{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisualClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(2048, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "        self.bn2 = nn.BatchNorm1d(4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_dim = 768, output_dim = 4, hidden_dim = 256):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudialClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudialClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 16)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(16, 4)\n",
    "        self.bn2 = nn.BatchNorm1d(4)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, criterion, device, num_epochs=50, patience=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Trains and validates the model.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The PyTorch model to train.\n",
    "    - dataloaders (dict): A dictionary containing 'train' and 'val' DataLoaders.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer to use for training.\n",
    "    - criterion (torch.nn.Module): The loss function.\n",
    "    - num_epochs (int): The number of epochs to train for.\n",
    "    - patience (int): The patience for early stopping.\n",
    "    \"\"\"\n",
    "    best_val_f1 = -float('inf')  \n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        val_probs = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaders['val']:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_probs.extend(outputs.cpu().numpy())\n",
    "\n",
    "\n",
    "        val_accuracy = np.mean(np.array(val_preds) == np.array(val_labels))\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='micro')\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1  \n",
    "            patience_counter = 0  \n",
    "            # print(f\"Validation F1 improved. Saving model...\")\n",
    "            # torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "            # print(f'Validation F1 did not improve. Patience: {patience_counter}/{patience}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break  \n",
    "\n",
    "\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}, Best Validation F1 Score: {best_val_f1:.4f}')\n",
    "    \n",
    "    return val_accuracy, best_val_f1, np.array(val_probs), np.array(val_preds), np.array(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_pool_features(df, feature_types, base_path=\"../data/\"):\n",
    "    \"\"\"\n",
    "    Extracts features from specified columns in the DataFrame, applies mean pooling,\n",
    "    and updates the DataFrame with new columns for these processed features.\n",
    "    \n",
    "    Args:\n",
    "    - df (DataFrame): The pandas DataFrame containing the features.\n",
    "    - feature_types (dict): A dictionary mapping from 'visual' and 'audio' to their respective column names in df.\n",
    "    - base_path (str): Base path where the feature files are stored.\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, column in feature_types.items():\n",
    "        features_list = []\n",
    "        for _, row in df.iterrows():\n",
    "            file_path = row[column]\n",
    "            features = np.load(f\"{base_path}{file_path}\")\n",
    "            features_list.append(np.mean(features, axis=0) if key != 'text' else features)\n",
    "        \n",
    "        df[f'extracted_{key}_features'] = features_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets_and_loaders(df, feature_columns, label_column='emotion_labels', batch_size = 4, test_size = 0.2):\n",
    "    \"\"\"\n",
    "    Prepares datasets and dataloaders for training and validation.\n",
    "    \n",
    "    Args:\n",
    "    - df (DataFrame): The pandas DataFrame containing the pooled features and labels.\n",
    "    - feature_columns (list): List of column names for the features to be used.\n",
    "    - label_column (str): The column name where the label data is stored.\n",
    "    - batch_size (int): Batch size for the dataloaders.\n",
    "    - test_size (float): Proportion of the dataset to include in the test split.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary of dataloaders for training and validation for each feature type.\n",
    "    \"\"\"\n",
    "\n",
    "    dataloaders = {}\n",
    "    y = torch.tensor(df[label_column].values, dtype = torch.long)\n",
    "\n",
    "    for feature_type in feature_columns:\n",
    "        X = np.array(df[feature_type].tolist(), dtype = np.float32)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train), y_train)\n",
    "        val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_val), y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
    "        \n",
    "        dataloaders[f'{feature_type}_train'] = train_loader\n",
    "        dataloaders[f'{feature_type}_val'] = val_loader\n",
    "\n",
    "    return dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Class Imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/csv/dataset.csv')\n",
    "\n",
    "labels = df['emotion_labels'].values\n",
    "classes = np.unique(labels)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "class_weights_tensor = class_weights_tensor.to('cuda')\n",
    "class_weights_tensor, class_weights_tensor.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAM GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'learning_rate': [0.0001, 0.001, 0.01],\n",
    "#     'optimizer': [optim.Adam],\n",
    "#     'criterion': [nn.CrossEntropyLoss],\n",
    "#     'epochs': [30, 50],\n",
    "#     'batch_size': [4, 16, 32],\n",
    "#     'patience': [5, 10, 15],\n",
    "#     'weight_decay': [0, 1e-4, 1e-2],\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4],\n",
    "    'optimizer': [optim.Adam],\n",
    "    'criterion': [nn.CrossEntropyLoss],\n",
    "    'epochs': [50],\n",
    "    'batch_size': [32],\n",
    "    'patience': [15],\n",
    "    'weight_decay': [0],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_class, parameters, lr, weight_decay, momentum=None):\n",
    "    if optimizer_class == optim.Adam:\n",
    "        return optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_class == optim.SGD:\n",
    "        # Ensure momentum is provided for SGD; otherwise, default to 0\n",
    "        return optim.SGD(parameters, lr=lr, momentum=momentum if momentum is not None else 0, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "def get_criterion(criterion_class):\n",
    "    if criterion_class == nn.CrossEntropyLoss:\n",
    "        return nn.CrossEntropyLoss()\n",
    "    elif criterion_class == nn.NLLLoss:\n",
    "        return nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(df, feature_columns, param_grid, device='cuda'):\n",
    "    max_vis_acc, max_aud_acc, max_text_acc = -np.inf, -np.inf, -np.inf\n",
    "    best_params_vis, best_params_aud, best_params_text = None, None, None\n",
    "\n",
    "    combinations = list(product(*param_grid.values()))\n",
    "\n",
    "    for combination in tqdm(combinations):\n",
    "        lr, optimizer_class, criterion_class, epochs, batch_size, patience, wd = combination\n",
    "        \n",
    "        dataloaders = prepare_datasets_and_loaders(df, feature_columns, batch_size=batch_size)\n",
    "        \n",
    "        model_vis = VisualClassifier().to(device)\n",
    "        optimizer_vis = optim.Adam(model_vis.parameters(), lr=lr, weight_decay=wd)\n",
    "        \n",
    "        model_aud = AudialClassifier().to(device)\n",
    "        optimizer_aud = optim.Adam(model_aud.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        model_text = TextClassifier().to(device)\n",
    "        optimizer_text = optim.Adam(model_text.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        \n",
    "        models_optimizers = {\n",
    "            'extracted_visual_features': (model_vis, optimizer_vis),\n",
    "            'extracted_audio_features': (model_aud, optimizer_aud),\n",
    "            'extracted_text_features': (model_text, optimizer_text),\n",
    "        }\n",
    "\n",
    "        for feature_type, (model, optimizer) in models_optimizers.items():\n",
    "            print(f\"\\nTraining {feature_type.split('_')[1].capitalize()} Model with lr={lr}, optimizer={optimizer_class.__name__}, criterion={criterion_class.__name__}, epochs={epochs}, batch_size={batch_size}, Patience={patience}, Weight decay={wd}\")\n",
    "            val_accuracy, best_val_f1, val_probs, val_preds, val_labels = train_model(\n",
    "                model, \n",
    "                {'train': dataloaders[f'{feature_type}_train'], 'val': dataloaders[f'{feature_type}_val']}, \n",
    "                optimizer, criterion, device=device, num_epochs=epochs, patience=patience\n",
    "            )\n",
    "\n",
    "            if feature_type == 'extracted_visual_features' and val_accuracy > max_vis_acc:\n",
    "                max_vis_acc = val_accuracy\n",
    "                best_params_vis = {'learning_rate': lr, 'optimizer': optimizer_class.__name__, 'criterion': criterion_class.__name__, 'epochs': epochs, 'batch_size': batch_size, 'patience': patience, 'weight_decay': wd, 'validation_accuracy': val_accuracy}\n",
    "            \n",
    "            elif feature_type == 'extracted_audio_features' and val_accuracy > max_aud_acc:\n",
    "                max_aud_acc = val_accuracy\n",
    "                best_params_aud = {'learning_rate': lr, 'optimizer': optimizer_class.__name__, 'criterion': criterion_class.__name__, 'epochs': epochs, 'batch_size': batch_size, 'patience': patience, 'weight_decay': wd, 'validation_accuracy': val_accuracy}\n",
    "            \n",
    "            elif feature_type == 'extracted_text_features' and val_accuracy > max_text_acc:\n",
    "                max_text_acc = val_accuracy\n",
    "                best_params_text = {'learning_rate': lr, 'optimizer': optimizer_class.__name__, 'criterion': criterion_class.__name__, 'epochs': epochs, 'batch_size': batch_size, 'patience': patience, 'weight_decay': wd, 'validation_accuracy': val_accuracy}\n",
    "\n",
    "    return best_params_vis, best_params_aud, best_params_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aud = AudialClassifier()\n",
    "model_aud = model_aud.to(device)\n",
    "\n",
    "model_vis = VisualClassifier()\n",
    "model_vis = model_vis.to(device)\n",
    "\n",
    "model_text = TextClassifier()\n",
    "model_text = model_text.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "\n",
    "feature_types = {'visual': 'visual_features', 'audio': 'acoustic_features', 'text':'lexical_features'}\n",
    "feature_columns = ['extracted_visual_features', 'extracted_audio_features','extracted_text_features']\n",
    "\n",
    "extract_and_pool_features(df, feature_types)\n",
    "dataloaders = prepare_datasets_and_loaders(df, feature_columns, batch_size = 4)\n",
    "\n",
    "optimizer_aud = optim.Adam(model_aud.parameters(), lr = 0.001, weight_decay = 1e-4)\n",
    "optimizer_vis = optim.Adam(model_vis.parameters(), lr = 0.001, weight_decay = 0)\n",
    "optimizer_text = optim.Adam(model_vis.parameters(), lr = 0.001, weight_decay = 1e-4)\n",
    "\n",
    "\n",
    "models_optimizers = {\n",
    "    'extracted_visual_features': (model_vis, optimizer_vis),\n",
    "    'extracted_audio_features': (model_aud, optimizer_aud),\n",
    "    'extracted_text_features': (model_text, optimizer_text),\n",
    "}\n",
    "\n",
    "model_outputs = {}\n",
    "for feature_type, (model, optimizer) in models_optimizers.items():\n",
    "    print(f\"Training with {feature_type}:\")\n",
    "    val_accuracy, best_val_f1, val_probs, val_preds, val_labels = train_model(\n",
    "        model, \n",
    "        {'train': dataloaders[f'{feature_type}_train'], 'val': dataloaders[f'{feature_type}_val']}, \n",
    "        optimizer, criterion, device = device, num_epochs = 30, patience = 15)\n",
    "\n",
    "    model_outputs[feature_type] = {\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'val_preds': val_preds,\n",
    "        'val_labels': val_labels\n",
    "    }\n",
    "\n",
    "len(model_outputs['extracted_text_features']['val_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_params, aud_params, text_params = grid_search(df, feature_columns, param_grid, device='cuda')\n",
    "print(\"Best Visual Model Params:\", vis_params)\n",
    "print(\"Best Audio Model Params:\", aud_params)\n",
    "print(\"Best Text Model Params:\", text_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_params , aud_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lack of alignment temporally and lack of similarity of shapes of data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexConcatModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=4):\n",
    "        super(ComplexConcatModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc5 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        \"\"\"\n",
    "        features: Numpy array of concatenated features.\n",
    "        labels: Numpy array of labels.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Concatenate features\n",
    "concatenated_features = np.hstack((\n",
    "    np.array(df['extracted_visual_features'].tolist()),\n",
    "    np.array(df['extracted_audio_features'].tolist()),\n",
    "    np.array(df['extracted_text_features'].tolist())\n",
    "))\n",
    "\n",
    "print(concatenated_features.shape)\n",
    "labels = df['emotion_labels'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(concatenated_features, labels, test_size=0.2, random_state=42)  ### USE 5-FOLD CROSS VALIDATION \n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter=1000).fit(X_train, y_train)\n",
    "yPred_clf = clf.predict(X_val)\n",
    "# print(accuracy_score(y_val, yPred_clf))\n",
    "\n",
    "\n",
    "train_dataset = ConcatDataset(X_train, y_train)\n",
    "val_dataset = ConcatDataset(X_val, y_val)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size = 16, shuffle = True),\n",
    "    'val': DataLoader(val_dataset, batch_size = 16, shuffle = False)\n",
    "}\n",
    "\n",
    "model = ComplexConcatModel(input_dim = concatenated_features.shape[1]).to(device)  \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "\n",
    "_, _, _, _, _ = train_model(\n",
    "    model = model,\n",
    "    dataloaders = dataloaders,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,  \n",
    "    device = device,\n",
    "    num_epochs = 50,\n",
    "    patience = 15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Late fusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_aud = model_outputs['extracted_audio_features']['val_preds']\n",
    "predictions_vis = model_outputs['extracted_visual_features']['val_preds']\n",
    "predictions_text = model_outputs['extracted_text_features']['val_preds']\n",
    "\n",
    "\n",
    "# final_predictions = (predictions_vis + predictions_aud + predictions_text) / 3\n",
    "weight_dict = {\n",
    "    'weight_vis': [i/100 for i in range(11, 100)],\n",
    "    'weight_aud': [i/100 for i in range(11, 100)],\n",
    "    'weight_text': [i/100 for i in range(11, 100)],\n",
    "}\n",
    "\n",
    "weight_combinations = list(product(*weight_dict.values()))\n",
    "max_params = -np.inf\n",
    "best_weights = None\n",
    "\n",
    "for weights in tqdm(weight_combinations):\n",
    "    # print(weights)\n",
    "    weight_vis, weight_aud, weight_text = weights\n",
    "    final_predictions_weighted = (weight_vis * predictions_vis + weight_aud * predictions_aud + weight_text * predictions_text)\n",
    "    # print(final_predictions_weighted)\n",
    "    final_predicted_classes = np.argmax(final_predictions_weighted)\n",
    "    acc = (np.mean(final_predicted_classes == model_outputs['extracted_text_features']['val_labels'] ))\n",
    "\n",
    "    if acc > max_params:\n",
    "        max_params = acc\n",
    "        best_weights = weights\n",
    "final_predictions_weighted.shape, final_predicted_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_aud = model_outputs['extracted_audio_features']['val_preds']\n",
    "predictions_vis = model_outputs['extracted_visual_features']['val_preds']\n",
    "predictions_text = model_outputs['extracted_text_features']['val_preds']\n",
    "\n",
    "\n",
    "# final_predictions = (predictions_vis + predictions_aud + predictions_text) / 3\n",
    "weight_vis, weight_aud, weight_text  = best_weights\n",
    "print(weight_vis, weight_aud, weight_text)\n",
    "final_predictions_weighted = (weight_vis * predictions_vis + weight_aud * predictions_aud + weight_text * predictions_text)\n",
    "final_predicted_classes = np.argmax(final_predictions_weighted, axis = 0)\n",
    "print(np.mean(final_predicted_classes == model_outputs['extracted_text_features']['val_labels'] ))\n",
    "\n",
    "# final_predicted_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.argmax(model_outputs['extracted_text_features']['val_preds'], axis=0) == model_outputs['extracted_text_features']['val_labels'] ))\n",
    "\n",
    "print(np.mean(np.argmax(model_outputs['extracted_visual_features']['val_preds'], axis=0) == model_outputs['extracted_text_features']['val_labels'] ))\n",
    "print(np.mean(np.argmax(model_outputs['extracted_audio_features']['val_preds'] , axis=0) == model_outputs['extracted_text_features']['val_labels'] ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Class1', 'Class2', 'Class3', 'Class4']\n",
    "\n",
    "for feature_type, outputs in model_outputs.items():\n",
    "    cm = confusion_matrix(outputs['val_preds'], outputs['val_labels'])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    modality = feature_type.split('_')[1]  # Extract modality name from feature_type\n",
    "    plt.title(f'Confusion Matrix for {modality.capitalize()} Model')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_for_fold(model, dataloaders, optimizer, criterion, device,feature_type, num_epochs=50, patience=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Trains the model for a single fold in the cross-validation setup.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The PyTorch model to train.\n",
    "    - dataloaders (dict): A dictionary containing 'train' and 'val' DataLoaders for the fold.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer to use for training.\n",
    "    - criterion (torch.nn.Module): The loss function.\n",
    "    - device (torch.device): The device to train the model on.\n",
    "    - num_epochs (int): The number of epochs to train for.\n",
    "    - patience (int): The patience for early stopping.\n",
    "    \n",
    "    Returns:\n",
    "    - fold_best_val_f1 (float): The best F1 score achieved on the validation set for the fold.\n",
    "    \"\"\"\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # print('dataloaders: ', dataloaders)\n",
    "        #for inputs, labels in dataloaders['extracted_visual_features_train']:\n",
    "        for inputs, labels in dataloaders[f'{feature_type}_train']:\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        val_labels = []\n",
    "        val_preds = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaders[f'{feature_type}_val']:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='micro')\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return best_val_f1\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr f1: 0.5298507462686567\n",
      "curr f1: 0.5340909090909091\n",
      "curr f1: 0.5454545454545454\n",
      "curr f1: 0.5454545454545454\n",
      "curr f1: 0.5871212121212122\n",
      "Mean F1 across folds: 0.5484 ± 0.0203\n",
      "Early stopping triggered at epoch 46\n",
      "curr f1: 0.5932835820895522\n",
      "curr f1: 0.5606060606060606\n",
      "curr f1: 0.571969696969697\n",
      "curr f1: 0.553030303030303\n",
      "curr f1: 0.6136363636363636\n",
      "Mean F1 across folds: 0.5785 ± 0.0222\n",
      "curr f1: 0.667910447761194\n",
      "curr f1: 0.6363636363636364\n",
      "curr f1: 0.6590909090909091\n",
      "curr f1: 0.6515151515151515\n",
      "curr f1: 0.6212121212121212\n",
      "Mean F1 across folds: 0.6472 ± 0.0166\n"
     ]
    }
   ],
   "source": [
    "model_names_dict = {'aud': AudialClassifier, 'vis' : VisualClassifier, 'text' : TextClassifier}\n",
    "model_names = VisualClassifier# AudialClassifier#, VisualClassifier, TextClassifier\n",
    "feature_model_dict = {'visual_features': ('extracted_visual_features', VisualClassifier), 'audio_features': ('extracted_audio_features', AudialClassifier), 'lexical_features': ('extracted_text_features', TextClassifier)}\n",
    "feature_columns = ['extracted_visual_features', 'extracted_audio_features','extracted_text_features']\n",
    "\n",
    "cross_validate_model( df, model_class = VisualClassifier, feature_type = 'extracted_visual_features', column='visual_features',  n_splits = 5)\n",
    "cross_validate_model( df, model_class = AudialClassifier, feature_type = 'extracted_audio_features', column='acoustic_features',  n_splits = 5)\n",
    "cross_validate_model( df, model_class = TextClassifier,   feature_type = 'extracted_text_features', column='lexical_features',  n_splits = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
