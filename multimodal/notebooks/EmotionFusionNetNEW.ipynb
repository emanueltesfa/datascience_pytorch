{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisualClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(2048, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "        self.bn2 = nn.BatchNorm1d(4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_dim = 768, output_dim = 4, hidden_dim = 256):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudialClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudialClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 16)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(16, 4)\n",
    "        self.bn2 = nn.BatchNorm1d(4)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, criterion, device, num_epochs=50, patience=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Trains and validates the model.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The PyTorch model to train.\n",
    "    - dataloaders (dict): A dictionary containing 'train' and 'val' DataLoaders.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer to use for training.\n",
    "    - criterion (torch.nn.Module): The loss function.\n",
    "    - num_epochs (int): The number of epochs to train for.\n",
    "    - patience (int): The patience for early stopping.\n",
    "    \"\"\"\n",
    "    best_val_f1 = -float('inf')  \n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        val_probs = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaders['val']:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_probs.extend(outputs.cpu().numpy())\n",
    "\n",
    "\n",
    "        val_accuracy = np.mean(np.array(val_preds) == np.array(val_labels))\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='micro')\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1  \n",
    "            patience_counter = 0  \n",
    "            # print(f\"Validation F1 improved. Saving model...\")\n",
    "            # torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "            # print(f'Validation F1 did not improve. Patience: {patience_counter}/{patience}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break  \n",
    "\n",
    "\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}, Best Validation F1 Score: {best_val_f1:.4f}')\n",
    "    \n",
    "    return val_accuracy, best_val_f1, np.array(val_probs), np.array(val_preds), np.array(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_pool_features(df, feature_types, base_path=\"../data/\"):\n",
    "    \"\"\"\n",
    "    Extracts features from specified columns in the DataFrame, applies mean pooling,\n",
    "    and updates the DataFrame with new columns for these processed features.\n",
    "    \n",
    "    Args:\n",
    "    - df (DataFrame): The pandas DataFrame containing the features.\n",
    "    - feature_types (dict): A dictionary mapping from 'visual' and 'audio' to their respective column names in df.\n",
    "    - base_path (str): Base path where the feature files are stored.\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, column in feature_types.items():\n",
    "        features_list = []\n",
    "        for _, row in df.iterrows():\n",
    "            file_path = row[column]\n",
    "            features = np.load(f\"{base_path}{file_path}\")\n",
    "            features_list.append(np.mean(features, axis=0) if key != 'text' else features)\n",
    "        \n",
    "        df[f'extracted_{key}_features'] = features_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets_and_loaders(df, feature_columns, label_column='emotion_labels', batch_size = 4, test_size = 0.2):\n",
    "    \"\"\"\n",
    "    Prepares datasets and dataloaders for training and validation.\n",
    "    \n",
    "    Args:\n",
    "    - df (DataFrame): The pandas DataFrame containing the pooled features and labels.\n",
    "    - feature_columns (list): List of column names for the features to be used.\n",
    "    - label_column (str): The column name where the label data is stored.\n",
    "    - batch_size (int): Batch size for the dataloaders.\n",
    "    - test_size (float): Proportion of the dataset to include in the test split.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary of dataloaders for training and validation for each feature type.\n",
    "    \"\"\"\n",
    "\n",
    "    dataloaders = {}\n",
    "    y = torch.tensor(df[label_column].values, dtype = torch.long)\n",
    "\n",
    "    for feature_type in feature_columns:\n",
    "        X = np.array(df[feature_type].tolist(), dtype = np.float32)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train), y_train)\n",
    "        val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_val), y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
    "        \n",
    "        dataloaders[f'{feature_type}_train'] = train_loader\n",
    "        dataloaders[f'{feature_type}_val'] = val_loader\n",
    "\n",
    "    return dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Class Imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0183, 1.0844, 1.8556, 0.6423], device='cuda:0'), torch.Size([4]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/csv/dataset.csv')\n",
    "\n",
    "labels = df['emotion_labels'].values\n",
    "classes = np.unique(labels)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "class_weights_tensor = class_weights_tensor.to('cuda')\n",
    "class_weights_tensor, class_weights_tensor.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAM GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'learning_rate': [0.0001, 0.001, 0.01],\n",
    "#     'optimizer': [optim.Adam],\n",
    "#     'criterion': [nn.CrossEntropyLoss],\n",
    "#     'epochs': [30, 50],\n",
    "#     'batch_size': [4, 16, 32],\n",
    "#     'patience': [5, 10, 15],\n",
    "#     'weight_decay': [0, 1e-4, 1e-2],\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4],\n",
    "    'optimizer': [optim.Adam],\n",
    "    'criterion': [nn.CrossEntropyLoss],\n",
    "    'epochs': [50],\n",
    "    'batch_size': [32],\n",
    "    'patience': [15],\n",
    "    'weight_decay': [0],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_class, parameters, lr, weight_decay, momentum=None):\n",
    "    if optimizer_class == optim.Adam:\n",
    "        return optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_class == optim.SGD:\n",
    "        # Ensure momentum is provided for SGD; otherwise, default to 0\n",
    "        return optim.SGD(parameters, lr=lr, momentum=momentum if momentum is not None else 0, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "def get_criterion(criterion_class):\n",
    "    if criterion_class == nn.CrossEntropyLoss:\n",
    "        return nn.CrossEntropyLoss()\n",
    "    elif criterion_class == nn.NLLLoss:\n",
    "        return nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(df, feature_columns, param_grid, device='cuda'):\n",
    "    max_vis_acc, max_aud_acc, max_text_acc = -np.inf, -np.inf, -np.inf\n",
    "    best_params_vis, best_params_aud, best_params_text = None, None, None\n",
    "\n",
    "    combinations = list(product(*param_grid.values()))\n",
    "\n",
    "    for combination in tqdm(combinations):\n",
    "        lr, optimizer_class, criterion_class, epochs, batch_size, patience, wd = combination\n",
    "        \n",
    "        dataloaders = prepare_datasets_and_loaders(df, feature_columns, batch_size=batch_size)\n",
    "        \n",
    "        model_vis = VisualClassifier().to(device)\n",
    "        optimizer_vis = optim.Adam(model_vis.parameters(), lr=lr, weight_decay=wd)\n",
    "        \n",
    "        model_aud = AudialClassifier().to(device)\n",
    "        optimizer_aud = optim.Adam(model_aud.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        model_text = TextClassifier().to(device)\n",
    "        optimizer_text = optim.Adam(model_text.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        \n",
    "        models_optimizers = {\n",
    "            'extracted_visual_features': (model_vis, optimizer_vis),\n",
    "            'extracted_audio_features': (model_aud, optimizer_aud),\n",
    "            'extracted_text_features': (model_text, optimizer_text),\n",
    "        }\n",
    "\n",
    "        for feature_type, (model, optimizer) in models_optimizers.items():\n",
    "            print(f\"\\nTraining {feature_type.split('_')[1].capitalize()} Model with lr={lr}, optimizer={optimizer_class.__name__}, criterion={criterion_class.__name__}, epochs={epochs}, batch_size={batch_size}, Patience={patience}, Weight decay={wd}\")\n",
    "            val_accuracy, best_val_f1, val_probs, val_preds, val_labels = train_model(\n",
    "                model, \n",
    "                {'train': dataloaders[f'{feature_type}_train'], 'val': dataloaders[f'{feature_type}_val']}, \n",
    "                optimizer, criterion, device=device, num_epochs=epochs, patience=patience\n",
    "            )\n",
    "\n",
    "            if feature_type == 'extracted_visual_features' and val_accuracy > max_vis_acc:\n",
    "                max_vis_acc = val_accuracy\n",
    "                best_params_vis = {'learning_rate': lr, 'optimizer': optimizer_class.__name__, 'criterion': criterion_class.__name__, 'epochs': epochs, 'batch_size': batch_size, 'patience': patience, 'weight_decay': wd, 'validation_accuracy': val_accuracy}\n",
    "            \n",
    "            elif feature_type == 'extracted_audio_features' and val_accuracy > max_aud_acc:\n",
    "                max_aud_acc = val_accuracy\n",
    "                best_params_aud = {'learning_rate': lr, 'optimizer': optimizer_class.__name__, 'criterion': criterion_class.__name__, 'epochs': epochs, 'batch_size': batch_size, 'patience': patience, 'weight_decay': wd, 'validation_accuracy': val_accuracy}\n",
    "            \n",
    "            elif feature_type == 'extracted_text_features' and val_accuracy > max_text_acc:\n",
    "                max_text_acc = val_accuracy\n",
    "                best_params_text = {'learning_rate': lr, 'optimizer': optimizer_class.__name__, 'criterion': criterion_class.__name__, 'epochs': epochs, 'batch_size': batch_size, 'patience': patience, 'weight_decay': wd, 'validation_accuracy': val_accuracy}\n",
    "\n",
    "    return best_params_vis, best_params_aud, best_params_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with extracted_visual_features:\n",
      "Validation Accuracy: 0.5075, Best Validation F1 Score: 0.5299\n",
      "Training with extracted_audio_features:\n",
      "Validation Accuracy: 0.5187, Best Validation F1 Score: 0.5933\n",
      "Training with extracted_text_features:\n",
      "Early stopping triggered\n",
      "Validation Accuracy: 0.2015, Best Validation F1 Score: 0.2239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_aud = AudialClassifier()\n",
    "model_aud = model_aud.to(device)\n",
    "\n",
    "model_vis = VisualClassifier()\n",
    "model_vis = model_vis.to(device)\n",
    "\n",
    "model_text = TextClassifier()\n",
    "model_text = model_text.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "\n",
    "feature_types = {'visual': 'visual_features', 'audio': 'acoustic_features', 'text':'lexical_features'}\n",
    "feature_columns = ['extracted_visual_features', 'extracted_audio_features','extracted_text_features']\n",
    "\n",
    "extract_and_pool_features(df, feature_types)\n",
    "dataloaders = prepare_datasets_and_loaders(df, feature_columns, batch_size = 4)\n",
    "\n",
    "optimizer_aud = optim.Adam(model_aud.parameters(), lr = 0.001, weight_decay = 1e-4)\n",
    "optimizer_vis = optim.Adam(model_vis.parameters(), lr = 0.001, weight_decay = 0)\n",
    "optimizer_text = optim.Adam(model_vis.parameters(), lr = 0.001, weight_decay = 1e-4)\n",
    "\n",
    "\n",
    "models_optimizers = {\n",
    "    'extracted_visual_features': (model_vis, optimizer_vis),\n",
    "    'extracted_audio_features': (model_aud, optimizer_aud),\n",
    "    'extracted_text_features': (model_text, optimizer_text),\n",
    "}\n",
    "\n",
    "model_outputs = {}\n",
    "for feature_type, (model, optimizer) in models_optimizers.items():\n",
    "    print(f\"Training with {feature_type}:\")\n",
    "    val_accuracy, best_val_f1, val_probs, val_preds, val_labels = train_model(\n",
    "        model, \n",
    "        {'train': dataloaders[f'{feature_type}_train'], 'val': dataloaders[f'{feature_type}_val']}, \n",
    "        optimizer, criterion, device = device, num_epochs = 30, patience = 15)\n",
    "\n",
    "    model_outputs[feature_type] = {\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'val_preds': val_preds,\n",
    "        'val_labels': val_labels\n",
    "    }\n",
    "\n",
    "len(model_outputs['extracted_text_features']['val_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Visual Model with lr=0.0001, optimizer=Adam, criterion=CrossEntropyLoss, epochs=50, batch_size=32, Patience=15, Weight decay=0\n",
      "Validation Accuracy: 0.5112, Best Validation F1 Score: 0.5112\n",
      "\n",
      "Training Audio Model with lr=0.0001, optimizer=Adam, criterion=CrossEntropyLoss, epochs=50, batch_size=32, Patience=15, Weight decay=0\n",
      "Validation Accuracy: 0.5224, Best Validation F1 Score: 0.5373\n",
      "\n",
      "Training Text Model with lr=0.0001, optimizer=Adam, criterion=CrossEntropyLoss, epochs=50, batch_size=32, Patience=15, Weight decay=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6306, Best Validation F1 Score: 0.6381\n",
      "Best Visual Model Params: {'learning_rate': 0.0001, 'optimizer': 'Adam', 'criterion': 'CrossEntropyLoss', 'epochs': 50, 'batch_size': 32, 'patience': 15, 'weight_decay': 0, 'validation_accuracy': 0.5111940298507462}\n",
      "Best Audio Model Params: {'learning_rate': 0.0001, 'optimizer': 'Adam', 'criterion': 'CrossEntropyLoss', 'epochs': 50, 'batch_size': 32, 'patience': 15, 'weight_decay': 0, 'validation_accuracy': 0.5223880597014925}\n",
      "Best Text Model Params: {'learning_rate': 0.0001, 'optimizer': 'Adam', 'criterion': 'CrossEntropyLoss', 'epochs': 50, 'batch_size': 32, 'patience': 15, 'weight_decay': 0, 'validation_accuracy': 0.6305970149253731}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vis_params, aud_params, text_params = grid_search(df, feature_columns, param_grid, device='cuda')\n",
    "print(\"Best Visual Model Params:\", vis_params)\n",
    "print(\"Best Audio Model Params:\", aud_params)\n",
    "print(\"Best Text Model Params:\", text_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_params , aud_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lack of alignment temporally and lack of similarity of shapes of data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexConcatModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=4):\n",
    "        super(ComplexConcatModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc5 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1336, 2944)\n",
      "Early stopping triggered\n",
      "Validation Accuracy: 0.6866, Best Validation F1 Score: 0.7351\n"
     ]
    }
   ],
   "source": [
    "class ConcatDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        \"\"\"\n",
    "        features: Numpy array of concatenated features.\n",
    "        labels: Numpy array of labels.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Concatenate features\n",
    "concatenated_features = np.hstack((\n",
    "    np.array(df['extracted_visual_features'].tolist()),\n",
    "    np.array(df['extracted_audio_features'].tolist()),\n",
    "    np.array(df['extracted_text_features'].tolist())\n",
    "))\n",
    "\n",
    "print(concatenated_features.shape)\n",
    "labels = df['emotion_labels'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(concatenated_features, labels, test_size=0.2, random_state=42)  ### USE 5-FOLD CROSS VALIDATION \n",
    "\n",
    "clf = LogisticRegression(random_state = 0, max_iter=1000).fit(X_train, y_train)\n",
    "yPred_clf = clf.predict(X_val)\n",
    "# print(accuracy_score(y_val, yPred_clf))\n",
    "\n",
    "\n",
    "train_dataset = ConcatDataset(X_train, y_train)\n",
    "val_dataset = ConcatDataset(X_val, y_val)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size = 16, shuffle = True),\n",
    "    'val': DataLoader(val_dataset, batch_size = 16, shuffle = False)\n",
    "}\n",
    "\n",
    "model = ComplexConcatModel(input_dim = concatenated_features.shape[1]).to(device)  \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "\n",
    "_, _, _, _, _ = train_model(\n",
    "    model = model,\n",
    "    dataloaders = dataloaders,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,  \n",
    "    device = device,\n",
    "    num_epochs = 50,\n",
    "    patience = 15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Late fusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/704969 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.11, 0.11, 0.11)\n",
      "[0.   0.11 0.44 0.22 0.22 0.22 0.11 0.66 0.33 0.66 0.55 0.88 0.77 0.55\n",
      " 0.55 0.55 0.77 0.66 0.22 0.44 0.77 0.44 0.66 0.22 0.77 0.44 0.33 0.77\n",
      " 0.66 0.22 0.77 0.22 0.11 0.22 0.66 0.77 0.88 0.33 0.   0.77 0.66 0.66\n",
      " 0.44 0.11 0.55 0.55 0.22 0.55 0.77 0.55 0.55 0.44 0.88 0.11 0.22 0.44\n",
      " 0.77 0.22 0.66 0.66 0.44 0.22 0.11 0.33 0.66 0.33 0.33 0.33 0.22 0.44\n",
      " 0.77 0.99 0.55 0.44 0.22 0.44 0.22 0.66 0.88 0.33 0.66 0.88 0.55 0.55\n",
      " 0.22 0.66 0.44 0.44 0.11 0.44 0.55 0.33 0.55 0.55 0.66 0.66 0.77 0.88\n",
      " 0.   0.22 0.77 0.66 0.66 0.66 0.44 0.66 0.55 0.22 0.55 0.66 0.88 0.88\n",
      " 0.33 0.88 0.33 0.77 0.22 0.44 0.77 0.77 0.44 0.55 0.33 0.44 0.66 0.66\n",
      " 0.77 0.88 0.55 0.22 0.77 0.66 0.22 0.33 0.55 0.66 0.77 0.88 0.66 0.66\n",
      " 0.55 0.22 0.33 0.22 0.44 0.22 0.11 0.88 0.77 0.55 0.66 0.88 0.88 0.55\n",
      " 0.88 0.22 0.77 0.55 0.55 0.44 0.22 0.44 0.44 0.66 0.33 0.44 0.44 0.88\n",
      " 0.22 0.55 0.33 0.66 0.44 0.33 0.66 0.77 0.99 0.88 0.44 0.55 0.88 0.11\n",
      " 0.   0.88 0.55 0.44 0.44 0.44 0.66 0.66 0.55 0.55 0.55 0.88 0.77 0.55\n",
      " 0.55 0.66 0.55 0.88 0.55 0.88 0.   0.44 0.66 0.22 0.44 0.44 0.77 0.66\n",
      " 0.44 0.44 0.66 0.66 0.11 0.88 0.66 0.22 0.44 0.44 0.33 0.22 0.22 0.55\n",
      " 0.44 0.66 0.33 0.44 0.77 0.44 0.77 0.22 0.66 0.77 0.22 0.55 0.55 0.\n",
      " 0.88 0.44 0.   0.88 0.22 0.55 0.44 0.22 0.77 0.77 0.55 0.22 0.33 0.22\n",
      " 0.22 0.88 0.44 0.77 0.44 0.55 0.77 0.33 0.22 0.77 0.44 0.   0.44 0.44\n",
      " 0.88 0.55]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m final_predictions_weighted \u001b[38;5;241m=\u001b[39m (weight_vis \u001b[38;5;241m*\u001b[39m predictions_vis \u001b[38;5;241m+\u001b[39m weight_aud \u001b[38;5;241m*\u001b[39m predictions_aud \u001b[38;5;241m+\u001b[39m weight_text \u001b[38;5;241m*\u001b[39m predictions_text)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_predictions_weighted)\n\u001b[1;32m---> 22\u001b[0m final_predicted_classes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_predictions_weighted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m acc \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(final_predicted_classes \u001b[38;5;241m==\u001b[39m model_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_text_features\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_labels\u001b[39m\u001b[38;5;124m'\u001b[39m] ))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acc \u001b[38;5;241m>\u001b[39m max_params:\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "predictions_aud = model_outputs['extracted_audio_features']['val_preds']\n",
    "predictions_vis = model_outputs['extracted_visual_features']['val_preds']\n",
    "predictions_text = model_outputs['extracted_text_features']['val_preds']\n",
    "\n",
    "\n",
    "# final_predictions = (predictions_vis + predictions_aud + predictions_text) / 3\n",
    "weight_dict = {\n",
    "    'weight_vis': [i/100 for i in range(11, 100)],\n",
    "    'weight_aud': [i/100 for i in range(11, 100)],\n",
    "    'weight_text': [i/100 for i in range(11, 100)],\n",
    "}\n",
    "\n",
    "weight_combinations = list(product(*weight_dict.values()))\n",
    "max_params = -np.inf\n",
    "best_weights = None\n",
    "\n",
    "for weights in tqdm(weight_combinations):\n",
    "    print(weights)\n",
    "    weight_vis, weight_aud, weight_text = weights\n",
    "    final_predictions_weighted = (weight_vis * predictions_vis + weight_aud * predictions_aud + weight_text * predictions_text)\n",
    "    print(final_predictions_weighted)\n",
    "    final_predicted_classes = np.argmax(final_predictions_weighted)\n",
    "    acc = (np.mean(final_predicted_classes == model_outputs['extracted_text_features']['val_labels'] ))\n",
    "\n",
    "    if acc > max_params:\n",
    "        max_params = acc\n",
    "        best_weights = weights\n",
    "final_predictions_weighted.shape, final_predicted_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m predictions_text \u001b[38;5;241m=\u001b[39m model_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_text_features\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_preds\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# final_predictions = (predictions_vis + predictions_aud + predictions_text) / 3\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m weight_vis, weight_aud, weight_text  \u001b[38;5;241m=\u001b[39m best_weights\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(weight_vis, weight_aud, weight_text)\n\u001b[0;32m      9\u001b[0m final_predictions_weighted \u001b[38;5;241m=\u001b[39m (weight_vis \u001b[38;5;241m*\u001b[39m predictions_vis \u001b[38;5;241m+\u001b[39m weight_aud \u001b[38;5;241m*\u001b[39m predictions_aud \u001b[38;5;241m+\u001b[39m weight_text \u001b[38;5;241m*\u001b[39m predictions_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "predictions_aud = model_outputs['extracted_audio_features']['val_preds']\n",
    "predictions_vis = model_outputs['extracted_visual_features']['val_preds']\n",
    "predictions_text = model_outputs['extracted_text_features']['val_preds']\n",
    "\n",
    "\n",
    "# final_predictions = (predictions_vis + predictions_aud + predictions_text) / 3\n",
    "weight_vis, weight_aud, weight_text  = best_weights\n",
    "print(weight_vis, weight_aud, weight_text)\n",
    "final_predictions_weighted = (weight_vis * predictions_vis + weight_aud * predictions_aud + weight_text * predictions_text)\n",
    "final_predicted_classes = np.argmax(final_predictions_weighted, axis = 1)\n",
    "print(np.mean(final_predicted_classes == model_outputs['extracted_text_features']['val_labels'] ))\n",
    "\n",
    "# final_predicted_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.argmax(model_outputs['extracted_text_features']['val_preds'], axis=1) == model_outputs['extracted_text_features']['val_labels'] ))\n",
    "\n",
    "print(np.mean(np.argmax(model_outputs['extracted_visual_features']['val_preds'], axis=1) == model_outputs['extracted_text_features']['val_labels'] ))\n",
    "print(np.mean(np.argmax(model_outputs['extracted_audio_features']['val_preds'] , axis=1) == model_outputs['extracted_text_features']['val_labels'] ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Class1', 'Class2', 'Class3', 'Class4']\n",
    "\n",
    "for feature_type, outputs in model_outputs.items():\n",
    "    cm = confusion_matrix(outputs['val_preds'], outputs['val_labels'])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    modality = feature_type.split('_')[1]  # Extract modality name from feature_type\n",
    "    plt.title(f'Confusion Matrix for {modality.capitalize()} Model')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
